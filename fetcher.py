import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from dotenv import load_dotenv
import re
import os
import openai
from db import create_database, insert_article

# Charger les variables d'environnement √† partir du fichier .env
load_dotenv()

# R√©cup√©rer la cl√© API depuis les variables d'environnement
openai.api_key = os.getenv("OPENAI_API_KEY")

# openai.api_key = "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"  # ‚ö†Ô∏è Remplace avec ta vraie cl√© OpenAI

# V√©rifie si une URL correspond √† un article
def is_valid_article_url(href):
    """V√©rifie si l'URL est valide pour un article."""
    return bool(re.search(r'/news/[\w-]+-\d+$', href)) or '/news/articles/' in href

# R√©cup√®re le vrai titre depuis la page de l‚Äôarticle
def extract_title_from_article(article_url):
    """Extrait le titre de l'article depuis l'URL de l'article."""
    try:
        response = requests.get(article_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        og_title = soup.find("meta", property="og:title")
        if og_title and og_title.get("content"):
            return og_title["content"].strip()
        title_tag = soup.find("title")
        if title_tag:
            return title_tag.get_text().strip().replace(" - BBC News", "")
    except Exception as e:
        print(f"Erreur en r√©cup√©rant le titre depuis {article_url} : {e}")
        return None

# V√©rifie si le titre est li√© au domaine des affaires
def is_business_related_with_trade(title):
    """V√©rifie si le titre est li√© au domaine des affaires, de l'√©conomie ou du commerce."""
    try:
        prompt = f"Le titre suivant est-il li√© au domaine des affaires, de l'√©conomie ou du commerce ? R√©ponds par OUI ou NON uniquement.\nTitre : \"{title}\""
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": "Tu es un assistant qui √©value si un titre est li√© au domaine des affaires, de l'√©conomie ou du commerce."},
                      {"role": "user", "content": prompt}],
            max_tokens=5,
            temperature=0
        )
        answer = response.choices[0].message.content.strip().lower()
        return "oui" in answer
    except Exception as e:
        print(f"‚ùå GPT Error: {e}")
        return False

# G√©n√®re un r√©sum√© de l‚Äôarticle avec GPT (une fois qu‚Äôil est valid√© comme li√© au domaine des affaires)
def generate_summary_with_gpt(content):
    """G√©n√®re un r√©sum√© de l'article √† l'aide de GPT."""
    try:
        prompt = f"Voici le contenu d‚Äôun article. Peux-tu me fournir un r√©sum√© clair et concis en 2 ou 3 phrases ?\n\n{content}"
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "system", "content": "Tu es un assistant qui r√©sume des articles de mani√®re professionnelle."},
                      {"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"‚ùå Erreur de r√©sum√© : {e}")
        return ""

# Extraction de la date de publication
def extract_publication_date(article_soup):
    """Extrait la date de publication depuis l'article."""
    try:
        meta_date = article_soup.find("meta", {"property": "article:published_time"})
        if meta_date and meta_date.get("content"):
            date_iso = meta_date["content"]
            date_only = date_iso.split("T")[0]
            return date_only
        meta_date2 = article_soup.find("meta", {"name": "OriginalPublicationDate"})
        if meta_date2 and meta_date2.get("content"):
            date_iso = meta_date2["content"]
            date_only = date_iso.split("T")[0]
            return date_only
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur d‚Äôextraction de date : {e}")
    return None

# R√©cup√®re les articles depuis la page d‚Äôaccueil BBC
def fetch_bbc_articles_from_website():
    """R√©cup√®re les articles d'actualit√© de BBC News li√©s aux affaires."""
    url = "https://www.bbc.com/news"
    response = requests.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'html.parser')
    articles = []
    seen = set()
    yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    # Parcours des liens d'articles sur la page d'accueil
    for link in soup.find_all('a', href=True):
        href = link['href']
        if is_valid_article_url(href):
            full_url = "https://www.bbc.com" + href if href.startswith("/") else href
            if full_url not in seen:
                seen.add(full_url)
                title = extract_title_from_article(full_url)
                if title and is_business_related_with_trade(title):
                    # R√©cup√®re le contenu complet de l'article pour g√©n√©rer un r√©sum√©
                    content = ""
                    try:
                        article_page = requests.get(full_url)
                        article_soup = BeautifulSoup(article_page.content, 'html.parser')
                        paragraphs = article_soup.find_all('p')
                        content = ' '.join(p.get_text() for p in paragraphs if p.get_text())
                    except Exception as e:
                        print(f"Erreur lors de la r√©cup√©ration du contenu de l'article : {e}")

                    summary = generate_summary_with_gpt(content) if content else ""
                    publication_date = extract_publication_date(article_soup) or datetime.now().strftime('%Y-%m-%d')
                    articles.append({
                        'title': title,
                        'url': full_url,
                        'publication_date': publication_date,
                        'summary': summary,
                        'source': 'BBC'
                    })

    return articles

if __name__ == "__main__":
    try:
        create_database()  # Cr√©er la base de donn√©es (si elle n'existe pas d√©j√†)
        articles = fetch_bbc_articles_from_website()  # R√©cup√©rer les articles
        if not articles:
            print("Aucun article trouv√©.")
        for article in articles:
            print(f"üìå Titre : {article['title']}")
            print(f"üîó URL : {article['url']}")
            print(f"üìÖ Date : {article['publication_date']}")
            print(f"üìù R√©sum√© : {article['summary']}")
            print(f"üè∑Ô∏è Source : {article['source']}")
            print("-" * 80)
            insert_article(article)  # Ins√©rer chaque article dans la base de donn√©es
    except Exception as e:
        print("Une erreur est survenue :", e)
